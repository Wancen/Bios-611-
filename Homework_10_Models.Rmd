---
title: "Homework10"
author: "Wancen Mu"
date: "November 28, 2018"
output: html_document
---


```{r include=FALSE}
  library(tidyverse)
library(modelr)
library(glmnet)
library(broom)
library(caret)
options(na.action = na.warn)
```


  This homework is due at 6 pm on Monday, December 3.  

  This set of exercise is largely taken from R for Data Science by Garrett Grolemund and Hadley Wickham.  

# Exercise 1

1.  One downside of the linear model is that it is sensitive to unusual values
    because the distance incorporates a squared term. Fit a linear model to 
    the simulated data below, and visualise the results. Rerun a few times to
    generate different simulated datasets. What do you notice about the model? 
    
    ```{r}
    set.seed(1)
    sim1a <- tibble(
      x = rep(1:10, each = 3),
      y = x * 1.5 + 6 + rt(length(x), df = 2)
    )
    ```

    Answer: We notice that a few abnormal values can force the fitted line deviate from the “intutively” best lines and generate distinct se.

    ```{r}
    sim1a_mod<-lm(y~x,data = sim1a)

    ggplot(aes(x,y),data = sim1a)+
      geom_point()+
      geom_abline(slope =sim1a_mod$coefficients[2], intercept =sim1a_mod$coefficients[1]  )
#Make a function to display different simulated datasets by rerunning several times
    simt <- function(i) {
  tibble(
    x = rep(1:10, each = 3),
    y = x * 1.5 + 6 + rt(length(x), df = 2),
    times = i
  )
}

sims <- map_df(1:9, simt) 
ggplot(sims, aes(x = x, y = y)) +
  geom_point() +
   geom_smooth(method = "lm", colour = "red") +
  facet_wrap(~times,ncol=3)
    ```

2.  There are multiple ways to get the MSE (mean squared error. i.e. 
    $\sum_{i=1}^{n} \frac {(y_i - \hat{y}_i)^2}{n - p}$ where $n$ is the sample  
    size, $p$ is the number of covariates plus 1). from an `lm` object.  
    Illustrate three different ways of getting the MSE from the following object.  
    
    ```{r}
    sim1_mod <- lm(y~x, data = modelr::sim1)
    ```
    
    Hint: 1) using `predict()`,   
          2) extracting `residual`,   
          3) using `summary()` to get some quantity related to MSE.  
          If you are not sure what the object contains, use `str()`.  

    Answer: 

    ```{r}
    mean((sim1$y-predict(sim1_mod))^2)
    mean(residuals(sim1_mod)^2)
    mean(summary(sim1_mod)$residuals^2)
    ```

# Exercise 2

1.  Instead of using `lm()` to fit a straight line, you can use `loess()`
    to fit a smooth curve. Repeat the process of model fitting, 
    grid generation, predictions, and visualisation on `modelr::sim1` using 
    `loess()` instead of `lm()`. How does the result compare to 
    `geom_smooth()`?
    
    Answer: Below plots the loess predictions and lm predictions. The loess produces a nonlinear, smooth line through the data and fits better comparing to straight line. Using `method=loess`in `geom_smooth` generate a same curve line as using `loess()` because the red line and cyan line are overlapped. Both residuals are distributed symmetric around 0.

    ```{r}
    sim1_loess<-loess(y~x,data=sim1)
    sim1_lm<-lm(y~x,data=sim1)
    grid_losses<-sim1 %>% data_grid(x)
    grid_losses<-grid_losses %>% add_predictions(sim1_loess)
    sim1_1 <- sim1 %>% add_residuals(sim1_loess)
    sim1_2 <- sim1 %>% add_residuals(sim1_lm)
    #plot the predictions with redline-loess(), blueline-lm()
    ggplot()+
      geom_point(aes(x,y),data = sim1)+
      geom_line(aes(x,y=pred),data = grid_losses,color="red",size=1)+
      geom_smooth(aes(x,y),data = sim1,method = "lm",color="blue",se=FALSE)+
      geom_smooth(aes(x,y),data = sim1,method = "loess",color="cyan",se=FALSE)
    ggplot(sim1, aes(x = x)) +
      geom_ref_line(h = 0) +
      geom_point(aes(y = resid),data = sim1_1,color="red") +
      geom_point(aes(y = resid), data=sim1_2, colour = "blue")


    ```
2.  What does `geom_ref_line()` do? What package does it come from?
    Why is displaying a reference line in plots showing residuals
    useful and important?
    
    Answer: The `geom_ref_line()` add a reference line to a plot. It comes from `modelr` package. It is important because it helps you to observe whether residuals are distributed centered at 0 with approximately the same variance which is a criteria to judge good models. In addition, it helps you detect the trend.

    ```{r}

    ```
    
3.  In high-dimensional settings, some extension to linear models have been   
    developed to control the effect of noise variables. LASSO, ridge regression, 
    and elastic nets are such examples. Fit usual linear model,
    LASSO, and ridge regression using `mtcars` data: i.e. `lm(mpg ~ ., mtcars)`
    for usual linear model. Use `glmnet::glmnet()` with
    `alpha = 1` for LASSO and `alpha = 0` for ridge regression.  
    To find the best amount of penalty (`lambda`), usually cross validation is 
    done, but here, simply put `lambda = 0.6`.  
    What is the main difference bewteen LASSO and ridge regression? (Look
    at their coefficients using `coef()` function. Keyword: sparsity)
    
    Answer: By looking at the coefficients of LASSo and ridge regression, we find that LASSo zero out the co-efficients of collinear variables whereas ridge regression can’t zero coefficients. Hence, it helps to select the variable(s) out of given n variables while performing lasso regression.

    ```{r}
    car_lm<-lm(mpg ~ ., mtcars)
    mtcars1<-mtcars %>% add_predictions(car_lm)
    x = model.matrix(mpg~., mtcars)[,-1]
    car_LASSo<-glmnet(x,mtcars$mpg,alpha=1,lambda = 0.6)
    car_ridge<-glmnet(x,mtcars$mpg,alpha=0,lambda = 0.6)
    coef(car_lm)
    coef(car_LASSo)
    coef(car_ridge)
    ```
    
# Exercise 3


1.  Write a model formula in `lm()` in R that represents the following formula:  
    $E[y|x_1, x_2] = \beta_0 + \beta_1x_1 + \beta_2x_1^2 + \beta_3x_1x_2 + \beta_4e^{x_1}$.  
    Note: Beware that there is no main effect term for $x_2$.  
    Use the following data (`dat`).  
    
    ```{r}
    set.seed(1)
    dat <- data_frame(x1 = rnorm(30), x2 = rnorm(30), y = x1*(x1-x2) - exp(x1)/3 + rnorm(30))
    ```
    
    
    Answer:
    
    ```{r}
    dat_mod <- lm(y ~ x1+I(x1^2)+I(x1*x2)+exp(x1), data = dat)
    #dat_mod <- lm(y ~ x1+I(x1^2)+x1:x2+exp(x1), data = dat)
    #model_matrix(dat, y ~ x1+I(x1^2)+I(x1*x2)+exp(x1))
    summary(dat_mod)
    ```

2.  Suppose for some reason, you do not want to include an intercept term in your
    linear model.  e.g. $E[y|x_1] = \beta_1x_1$.
    
    A. How would you remove the intercept in `lm()`? Write a code for such a model
    (Use the same dataset (`dat`)). 
    
    Answer:  
    
    ```{r}
    lm(y~x1-1,data = dat)

    ```

    
    B. Compare and contrast models with and without the intercept term using  
       `modelr::sim1`.  Plot the predicted values for comaparison.  
     
    Answer:  By ploting two graphs, we can find more points lie close to fitting lines with intercept which may return smaller residual and it has smaller slop. It's hard to say which one is better by eyes.
    
    ```{r}
    mod1a <- lm(y ~ x, data = sim1)
    mod1b <- lm(y ~ x - 1, data = sim1)

grid <- sim1 %>% 
  data_grid(x) %>% 
  gather_predictions(`with intercept`=mod1a,`without intercept`=mod1b)

ggplot(sim1, aes(x)) +
  geom_point(aes(y = y)) +
  geom_line(aes(y = pred), data = grid, colour = "red", size = 1)+
  facet_wrap(~model)

    ```
  
    C. Repeat part B using data `modelr::sim2`.
    
    Answer: Below two looks exactly the same because they both predict the mean value for each category.
    
    ```{r}
   mod2a <- lm(y ~ x, data = sim2)
    mod2b <- lm(y ~ x - 1, data = sim2)

grid <- sim2 %>% 
  data_grid(x) %>% 
  gather_predictions(`with intercept`=mod2a,`without intercept`=mod2b)

ggplot(sim2, aes(x)) +
  geom_point(aes(y = y)) +
   geom_point(data = grid, aes(y = pred), colour = "red", size = 4)+
  facet_wrap(~model)

    ```


# Exercise 4

1.  Using the data `daily` given below, do the following.  
    Create a new variable that splits the `wday` variable into terms, but only
    for Saturdays, i.e. it should have `Thurs`, `Fri`, but `Sat-summer`, 
    `Sat-spring`, `Sat-fall`. How does this model (`lm()` with `n` as the outcome) 
    compare with the model with every combination of `wday` and `term`?
    
    ```{r}
    term <- function(date) {
      cut(date, breaks = lubridate::ymd(20130101, 20130605, 20130825, 20140101),
          labels = c("spring", "summer", "fall"))
      }
    daily <- nycflights13::flights %>% 
      mutate(date = lubridate::make_date(year, month, day)) %>% 
      group_by(date) %>% 
      summarise(n = n()) %>% 
      mutate(wday = lubridate::wday(date, label = TRUE),
             term = term(date))
    ```
    
    Answer: Looking at the line graph below, we can find the model with combination of `wday` and `term` has higher residuals in spring and lower in summer compared to the saturday model. We still cannot distinguish which one is better, so we use `glance()` to look at their summary and find that saturday model is better because it has a lower r-squared and higher AIC, which is an estimate of the out of sample error.

    ```{r}
    daily <- daily %>%
  mutate(wday2 = ifelse(wday == 'Sat',paste0(wday,"-",term),as.character(wday)))
    mod4a <- lm(n ~ wday * term, data = daily)
    mod4b <- lm(n ~ wday2, data = daily)
    daily2<-daily %>% 
  spread_residuals(all_term = mod4a, sat_term = mod4b) %>% 
      mutate(resid_diff=all_term-sat_term)
  ggplot(data = daily2, aes(date, resid_diff)) +
    geom_line(alpha = 0.75) 
    glance(mod4a)
    glance(mod4b)
    ```

2.  Create a new `wday` variable that combines the day of week, term 
    (for Saturdays), and public holidays. What do the residuals of 
    that model look like?

    Answer: In the first plot, all residuals locate close to 0 except for spring term which maybe because we only include federal holidays in the United States in 2013 when there are other widely holidays, eg.Mothers Day, Fathers Day. The second plor tells the residuals difference mostly close to 0 except for 10 peaks because the impact of 10 holidays. Hence, the model combing holidays looks better.

    ```{r}
    holidays_2013 <-
  tribble(
    ~ holiday,                    ~ date,
    "New Year's Day",             20130101,
    "Martin Luther King Jr. Day", 20130121,
    "Washington's Birthday",      20130218,
    "Memorial Day",               20130527,
    "Independence Day",           20130704,
    "Labor Day",                  20130902,
    "Columbus Day",               20131028,
    "Veteran's Day",              20131111,
    "Thanksgiving",               20131128,
    "Christmas",                  20131225
  ) %>%
  mutate(date = lubridate::ymd(date))
    daily<-daily %>% 
      mutate(wday3=ifelse((date - 1L) %in% holidays_2013$date,"day after holiday",
                          ifelse((date + 1L) %in% holidays_2013$date,"day before holiday",
                                 ifelse(date %in% holidays_2013$date,"holiday",wday2))))
    mod4c <- lm(n ~ wday3, data = daily)
    daily2<-daily %>%
  spread_residuals(resid_sat_terms = mod4b, resid_holidays = mod4c) %>%
  mutate(resid_diff = resid_holidays - resid_sat_terms) 
    ggplot(data=daily2,aes(date,resid_holidays))+
    geom_point()
  ggplot(data=daily2,aes(date, resid_diff)) +
    geom_line(alpha = 0.75)
  
    ```

3.  What happens if you fit a day of week effect that varies by month 
    (i.e. `n ~ wday * month`)? Why is this not very helpful? 

    Answer: Comparing the residuals of `wday*term` and `wday*month`, we can see residuals of `wday*month` are higher during summer. The reason of that is month information is part of the date information. There model has only 4–5 observations to estimate each (month, weekday) combination parameter since only there only 4–5 weekdays in given month. Any estimates will be very uncertain. so the performance will be actually a little bit worse.

    ```{r}
    daily <- mutate(daily, month = lubridate::month(date))
    mod4d<-lm(n~wday*month,data = daily)
    daily %>% 
      gather_residuals(`wday*term`=mod4a,`wday*month`=mod4d) %>% 
      ggplot(aes(wday,resid,color = model))+
  geom_point(alpha = 0.75)+
  facet_wrap(~month)

    ```

4.  What would you expect the model `n ~ wday + ns(date, 5)` to look like?
    Knowing what you know about the data, why would you expect it to be
    not particularly effective?

    Answer: It will estimate a smooth seasonal trend (ns(date, 5)) with a day of the week cyclicality, (wday). Splines does better job when n changes slowly among Monday to Friday, but it has a systematic overestimation in summer and fall when weekday is Saturday. Hence, it shouldn’t be particularly effective.

    ```{r}
    mod4e <- lm(n ~ wday + splines::ns(date, 5), data = daily)
    #model_matrix(n ~ wday + splines::ns(date, 5),data = daily)
    grid4e<-daily %>% 
      data_grid(wday, date = seq_range(date, n = 13)) %>% 
      add_predictions(mod4e)
      ggplot(daily,aes(date,n,colour = wday))+
      geom_point()+
      geom_line(data=grid4e, aes(date,pred,colour = wday))
      
daily %>%
  gather_residuals (`wday*term`=mod4a,`wday+ns(date,5)`=mod4e)%>%
  arrange(date)%>%
  ggplot(aes(date,resid,color = model))+
  geom_line(alpha = 0.75)

    ```

5.  It's a little frustrating that Sunday and Saturday are on separate ends
    of the plot. Write a small function to set the levels of the 
    factor so that the week starts on Monday.

    Answer: 

    ```{r}
    relevel <- function(x) {
  forcats::fct_relevel(x, levels(x)[-1])  
}
  ggplot(daily,aes(relevel(wday),n))+
    geom_boxplot()+
    labs(x="Day of week", y="Number of flights")
    ```


# Exercise 5 `caret`

1.  Using the `iris` dataset, find which machine learning algorithm gives
    the best prediction accuracy for predicting `Species` among `knn`, 
    `rf` (randomForest), and `nnet` (neuralnet).  
    
    Show a) how you partition the data (training and test sets), b) how you
    train the models, and c) how you evaluate the models.  
    
    Fix the seed number within each chunk as needed for replication.  
    You do not have to customize the tuning parameters (i.e. use the default set).  
    Standardize the dataset before training using `preProcess()` or 
    `train(..., preProcess = ...)`.  

     
    Answer: By doing the tests below when we set 50% the percentage of data that goes to training and the rest 50% goes to test, we can see `nnet` models have the highest accuracy(0.986) than `knn`(0.96) and `rf`(0.96). Hence, the neuralnet machine learning algorithm gives the best prediction accuracy.

    ```{r}
    set.seed(0)
    preProcValues <- preProcess(iris, method = c("knnImpute","center","scale"))
    iris2 <- predict(preProcValues, iris)
    #partitioning data
    index <- createDataPartition(iris2$Species, p=0.5, list=FALSE)
    tr <- iris2[index,]
  te <- iris2[-index,]
  RFE <- rfe(x = tr %>% select(- Species), 
             y = tr$Species, 
             rfeControl = rfeControl(functions = rfFuncs, method = "repeatedcv",
                                     repeats = 3, verbose = FALSE))
  #RFE command to use all 4 variables stored in optVariables, y is outcome vector
  x <- RFE$optVariables[1:4]
  y <- "Species"
```

*Construct models*
```{r results='hide'}
#training models
set.seed(0)
  model.knn <- train(tr[, x], tr[, y], method = "knn")
  set.seed(0)
  model.rf <- train(tr[, x], tr[, y], method = "rf")
  set.seed(0)
  model.nnet <- train(tr[, x], tr[, y], method = "nnet")
```
*Prediction*
```{r}
  varImp(model.knn)
varImp(model.rf)
varImp(model.nnet)
pred.knn <- predict.train(object = model.knn, te[, x], type = "raw")
  table(pred.knn)
  confusionMatrix(pred.knn, te[, y])
  pred.rf <- predict.train(object = model.rf, te[, x], type = "raw")
  table(pred.rf)
  confusionMatrix(pred.rf, te[, y])
  pred.nnet <- predict.train(object = model.nnet, te[, x], type = "raw")
  table(pred.nnet)
  confusionMatrix(pred.nnet, te[, y])
```


